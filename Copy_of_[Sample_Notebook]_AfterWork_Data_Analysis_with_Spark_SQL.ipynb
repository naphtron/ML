{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uEgNLbV7veiA",
        "Yhm7TT6UvhfB",
        "NFG1NxKrv35H",
        "nT5V7zqCQ5-r",
        "881qMba0ROvo",
        "wiid-DCETRdf",
        "H1OnioiRTjns",
        "mKwHDlwuJ3Zk",
        "yI2HbiZOwkhe",
        "KAjuKCnpHktW",
        "t28O-l2wIM8v",
        "4azeF_O_wH6O",
        "Mz3t2x44wTBL",
        "08VUTD8TM8dG"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naphtron/ML/blob/main/Copy_of_%5BSample_Notebook%5D_AfterWork_Data_Analysis_with_Spark_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Sample Notebook] AfterWork: Data Analysis with Spark SQL"
      ],
      "metadata": {
        "id": "umq_uSWJvTLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-requisites"
      ],
      "metadata": {
        "id": "uEgNLbV7veiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PySpark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "7XGXuq5lDcU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3fc9135-d475-48c4-f964-748e125cf042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=042fb2f2954e8b8d3e21a51ca90263841acf3a7a353c17ba3a7ae0aebc75b10d\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hk9aHrLu1Ca"
      },
      "outputs": [],
      "source": [
        "# Data Importation\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can download all the datasets for this notebook [here](https://drive.google.com/file/d/1df4AAwSgWOrUH7zXSmO2RdDldJgW3uxf/view?usp=sharing)."
      ],
      "metadata": {
        "id": "JTkzZjeHYie8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Creating DataFrames\n"
      ],
      "metadata": {
        "id": "Yhm7TT6UvhfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create DataFrames to represent structured data in a tabular format, allowing us to perform data analysis efficiently. For example, we can use DataFrames to analyze sales data, track customer information, or process log files.\n",
        "\n"
      ],
      "metadata": {
        "id": "Jf0pJ0oOvvpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session with an application name\n",
        "spark = SparkSession.builder.appName('DataFrameExample').getOrCreate()\n",
        "\n",
        "# Load data from the CSV file into a DataFrame: https://afterwork.ai/ds/e/information_technology_pafch.csv\n",
        "df = spark.read.csv('information_technology_pafch.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Register the DataFrame as a temporary SQL view\n",
        "df.createOrReplaceTempView('servers')\n",
        "\n",
        "# Display the data\n",
        "df.show(5)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "V3kY3kOuvQyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge"
      ],
      "metadata": {
        "id": "NFG1NxKrv35H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a DataFrame using the dataset from the following URL: https://afterwork.ai/ds/ch/operations_y14ap.csv.  "
      ],
      "metadata": {
        "id": "Wrq_yAYlv8dC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session with an application name\n",
        "spark = SparkSession.builder.appName('DataFrameExample').getOrCreate()\n",
        "\n",
        "# Load data from the CSV file into a DataFrame\n",
        "df = spark.read.csv('operations_y14ap.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Register the DataFrame as a temporary SQL view\n",
        "# Write your code here\n",
        "\n",
        "# Display the data\n",
        "# Write your code here\n",
        "\n",
        "# Stop the Spark session\n",
        "# Write your code here"
      ],
      "metadata": {
        "id": "RrGo4hD5wAnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Type Conversion"
      ],
      "metadata": {
        "id": "nT5V7zqCQ5-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform data type conversion to convert the data types of columns in a DataFrame. For example, we may need to convert a string column to a numeric type for mathematical operations. A real-life use case for data type conversion is when working with financial data where we need to convert currency values from strings to numeric types for calculations. To apply data type conversion in SQLContext, we use functions like `cast()` to explicitly convert the data types of columns in a DataFrame.\n",
        "\n"
      ],
      "metadata": {
        "id": "0n2sMG43RMn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session with an application name\n",
        "spark = SparkSession.builder.appName('Data_Type_Conversion').getOrCreate()\n",
        "\n",
        "# Load data from the CSV file into a DataFrame: https://afterwork.ai/ds/e/servers_ame42.csv\n",
        "df = spark.read.csv('servers_ame42.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Display the data\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "2VbwvH3gRO9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a temporary SQL view\n",
        "df.createOrReplaceTempView('servers_table')\n",
        "\n",
        "# Perform the data type conversion in SQLContext\n",
        "converted_df = spark.sql(\"SELECT *, CAST(`Memory (GB)` AS INT) AS Memory_Int, CAST(`Storage (TB)` AS FLOAT) AS Storage_Float FROM servers_table\")\n",
        "\n",
        "# Display the filtered data\n",
        "converted_df.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "MtENxgwjVO3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge"
      ],
      "metadata": {
        "id": "881qMba0ROvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the dataset from the URL: https://afterwork.ai/ds/ch/servers_dig70.csv, write code to convert the 'Memory (GB)' column from string to integer data type in a DataFrame.\n"
      ],
      "metadata": {
        "id": "YV2-D4_ZS6gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session with an application name\n",
        "spark = SparkSession.builder.appName('Data_Type_Conversion').getOrCreate()\n",
        "\n",
        "# Load data from the CSV file into a DataFrame: https://afterwork.ai/ds/ch/servers_dig70.csv\n",
        "df = spark.read.csv('servers_dig70.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Display the data\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "EJKXrSbvSmyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a temporary SQL view\n",
        "# Write your code here\n",
        "\n",
        "# Perform the data type conversion in SQLContext\n",
        "# Write your code here\n",
        "\n",
        "# Display the filtered data\n",
        "# Write your code here\n",
        "\n",
        "# Stop the Spark session\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "V-td33JFVQbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Handling Duplicates"
      ],
      "metadata": {
        "id": "wiid-DCETRdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can handle duplicates by identifying and removing rows that have identical values across all columns. In real-life scenarios, duplicates can occur due to data entry errors, system issues, or merging multiple datasets. By handling duplicates, we prevent skewed results and maintain the quality of our analysis. To apply this concept, we first identify duplicate rows using the 'dropDuplicates' function.\n",
        "\n"
      ],
      "metadata": {
        "id": "cmHo4NN7TT2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session with an application name\n",
        "spark = SparkSession.builder.appName('HandlingDuplicates').getOrCreate()\n",
        "\n",
        "# Load data from the CSV file into a DataFrame: https://afterwork.ai/ds/e/sales_wmtgv.csv\n",
        "df = spark.read.csv('sales_wmtgv.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Display the data\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "VOCWBYtKTTm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a temporary SQL view\n",
        "df.createOrReplaceTempView('sales_data')\n",
        "\n",
        "# Perform the data analysis technique to handle duplicates\n",
        "filtered_df = spark.sql('SELECT * FROM sales_data').dropDuplicates()\n",
        "\n",
        "# Display the filtered data\n",
        "filtered_df.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "i6gsLoTaVSAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge"
      ],
      "metadata": {
        "id": "H1OnioiRTjns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the dataset from the URL: https://afterwork.ai/ds/ch/sales_7fdtc.csv, write code to handle duplicates by removing rows that have identical values across all columns. Use the 'dropDuplicates' function to achieve this.  \n"
      ],
      "metadata": {
        "id": "w18Mn6K_TmgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the SparkSession class from the pyspark.sql module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session with an application name\n",
        "spark = SparkSession.builder.appName('HandlingDuplicates').getOrCreate()\n",
        "\n",
        "# Load data from the CSV file into a DataFrame: https://afterwork.ai/ds/ch/sales_7fdtc.csv\n",
        "df = spark.read.csv('sales_7fdtc.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Display the data\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "x2CJ5P8FTsTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a temporary SQL view\n",
        "# Write your code here\n",
        "\n",
        "# Perform the data analysis technique to handle duplicates\n",
        "# Write your code here\n",
        "\n",
        "# Display the filtered data\n",
        "# Write your code here\n",
        "\n",
        "# Stop the Spark session\n",
        "# Write your code here"
      ],
      "metadata": {
        "id": "d8uWpdmGVTmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Handling Missing Values"
      ],
      "metadata": {
        "id": "mKwHDlwuJ3Zk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We handle missing values by identifying and dealing with any null or NaN values present in the dataset. Missing values can occur due to various reasons such as data entry errors, sensor malfunctions, or incomplete data collection. It is crucial to address missing values as they can impact the accuracy and reliability of our analysis. For example, in a dataset of customer feedback, missing values in the 'rating' column can skew our analysis of customer satisfaction.\n",
        "\n"
      ],
      "metadata": {
        "id": "a5WzYTB3J6RW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session with an application name\n",
        "spark = SparkSession.builder.appName('HandleMissingValues').getOrCreate()\n",
        "\n",
        "# Load data from the CSV file into a DataFrame: https://afterwork.ai/ds/e/operations_fx6ye.csv\n",
        "df = spark.read.csv('operations_fx6ye.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Display the data\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "bDgyUtkdJ8Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a temporary SQL view\n",
        "df.createOrReplaceTempView('servers')\n",
        "\n",
        "# Perform the data analysis technique - Handling Missing Values in SQLContext DataFrames\n",
        "df_cleaned = spark.sql('SELECT * FROM servers WHERE `CPU Cores` IS NULL AND `Storage (TB)` IS NOT NULL')\n",
        "\n",
        "# Display the filtered data\n",
        "df_cleaned.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "XwzvAsoFVG2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Filtering Data"
      ],
      "metadata": {
        "id": "yI2HbiZOwkhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We filter data to extract only the rows that meet specific conditions. For example, we may want to filter a DataFrame to only include records where the sales amount is greater than $1000. A real-life use case for filtering data is in e-commerce analytics, where we can filter customer data to identify high-spending customers based on their purchase history."
      ],
      "metadata": {
        "id": "Hx3m3pqQHVbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session with an application name\n",
        "spark = SparkSession.builder.appName('SalesAnalysis').getOrCreate()\n",
        "\n",
        "# Load data from the CSV file into a DataFrame: https://afterwork.ai/ds/e/sales_iaz2h.csv\n",
        "df = spark.read.csv('sales_iaz2h.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Display the data\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "BW_a5BWMHYdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a temporary SQL view\n",
        "df.createOrReplaceTempView('sales')\n",
        "\n",
        "# Perform the data analysis technique - Filtering data to extract only rows where Memory is greater than 64 GB\n",
        "filtered_data = spark.sql('SELECT * FROM sales WHERE `Memory (GB)` > 64')\n",
        "\n",
        "# Display the filtered data\n",
        "filtered_data.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "gednSaTYU_rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenge"
      ],
      "metadata": {
        "id": "KAjuKCnpHktW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the dataset from the URL: https://afterwork.ai/ds/ch/sales_9uorv.csv, write code to filter the DataFrame to only include records where the total price is greater than $1000.\n"
      ],
      "metadata": {
        "id": "i4LJXWiPHqod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session with an application name\n",
        "spark = SparkSession.builder.appName('SalesAnalysis').getOrCreate()\n",
        "\n",
        "# Load data from the CSV file into a DataFrame: https://afterwork.ai/ds/ch/sales_9uorv.csv\n",
        "df = spark.read.csv('sales_9uorv.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Display the data\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "k7v6SfsrHrqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a temporary SQL view\n",
        "# Write your code here\n",
        "\n",
        "# Perform the data analysis technique - Filtering data to extract only rows where total price is greater than $1000\n",
        "# Write your code here\n",
        "\n",
        "# Display the filtered data\n",
        "# Write your code here\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "BgIJf9bXVBrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Sorting Data"
      ],
      "metadata": {
        "id": "t28O-l2wIM8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We sort data to arrange it in a specific order based on one or more columns. This helps us organize the data in a meaningful way for analysis and presentation. For example, in a sales dataset, we can sort the data based on the sales amount in descending order to identify the top-selling products.\n",
        "\n"
      ],
      "metadata": {
        "id": "fotn3tNeIOFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session with an application name\n",
        "spark = SparkSession.builder.appName('SortingDataExample').getOrCreate()\n",
        "\n",
        "# Load data from the CSV file into a DataFrame: https://afterwork.ai/ds/e/operations_mtfg0.csv\n",
        "df = spark.read.csv('operations_mtfg0.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Display the data\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "iCMstVuGIRNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a temporary SQL view\n",
        "df.createOrReplaceTempView('operations_data')\n",
        "\n",
        "# Perform the data analysis technique - Sorting data by 'Memory (GB)' column in descending order\n",
        "sorted_data = spark.sql('SELECT * FROM operations_data ORDER BY `Memory (GB)` DESC')\n",
        "\n",
        "# Display the filtered data\n",
        "sorted_data.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "QpsirRYMVDa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Aggregating Data"
      ],
      "metadata": {
        "id": "4azeF_O_wH6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregating data involves combining and summarizing multiple rows of data into a single result. For example, we can aggregate sales data to calculate the total revenue generated by each product category. A real-life use case of aggregating data is in financial analysis, where we can aggregate transaction data to calculate total expenses or revenue for a specific time period. To apply aggregation, we use functions like SUM, AVG, COUNT, and MAX along with the GROUP BY clause to group the data based on a specific column.\n",
        "\n"
      ],
      "metadata": {
        "id": "LjWK9akXwOe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Create a Spark session with an application name\n",
        "spark = SparkSession.builder.appName('SalesAnalysis').getOrCreate()\n",
        "\n",
        "# Load data from the CSV file into a DataFrame: https://afterwork.ai/ds/e/sales_ew6oq.csv\n",
        "df = spark.read.csv('sales_ew6oq.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Display the data\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "sWSqiU6DwQsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a temporary SQL view\n",
        "df.createOrReplaceTempView('sales')\n",
        "\n",
        "# Perform the data analysis technique - Aggregating data to calculate total revenue generated by each product category\n",
        "result = spark.sql('SELECT Location, SUM(`Storage (TB)`) AS Total_Storage FROM sales GROUP BY Location')\n",
        "\n",
        "# Display the filtered data\n",
        "result.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "xKcAoyPAU057"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenge"
      ],
      "metadata": {
        "id": "Mz3t2x44wTBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the dataset from the URL: https://afterwork.ai/ds/ch/sales_6on5e.csv, write a SQL query to find the average price generated for each product category. Use aggregation functions like AVG and GROUP BY clause to achieve this."
      ],
      "metadata": {
        "id": "AoifxyarwUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Create a Spark session with an application name\n",
        "spark = SparkSession.builder.appName('SalesAnalysis').getOrCreate()\n",
        "\n",
        "# Load data from the CSV file into a DataFrame: https://afterwork.ai/ds/ch/sales_6on5e.csv\n",
        "df = spark.read.csv('sales_6on5e.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Display the data\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "QjmNi7bwwW9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a temporary SQL view\n",
        "# Write your code here\n",
        "\n",
        "# Perform the data analysis technique - Aggregating data to calculate total price generated by each product category\n",
        "# Write your code here\n",
        "\n",
        "# Display the filtered data\n",
        "# Write your code here\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "eMwzkD0mU9T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Grouping Data by Multiple Columns"
      ],
      "metadata": {
        "id": "08VUTD8TM8dG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We group data by multiple columns when we want to analyze data based on multiple criteria. For example, in a sales dataset, we may want to group sales data by both region and product category to understand which regions perform best for each category. This allows us to perform aggregate functions on the grouped data and generate meaningful summaries for our analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZD0mz5GfM-oI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session with an application name\n",
        "spark = SparkSession.builder.appName('SalesAnalysis').getOrCreate()\n",
        "\n",
        "# Load data from the CSV file into a DataFrame: https://afterwork.ai/ds/e/sales_pvaqb.csv\n",
        "df = spark.read.csv('sales_pvaqb.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Display the data\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "X9Va0WMfM1Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a temporary SQL view\n",
        "df.createOrReplaceTempView('sales_data')\n",
        "\n",
        "# Perform the data analysis technique by grouping data by multiple columns\n",
        "grouped_data = spark.sql('SELECT Location, Environment, COUNT(*) AS TotalServers FROM sales_data GROUP BY Location, Environment')\n",
        "\n",
        "# Display the filtered data\n",
        "grouped_data.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "ukG_9iiUVKad"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}